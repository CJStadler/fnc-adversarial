\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{CS6140 Final Project Milestone Report}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Joshua Bundt \\
  Department of Computer Science\\
  Northeastern University\\
  Boston, MA \\
  \texttt{bundt.j@husky.neu.edu} \\
  %% examples of more authors
  \And
  Christopher Stadler \\
  Department of Computer Science \\
  Northeastern University \\
  Boston, MA \\
  \texttt{stadler.c@husky.nedu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{Introduction}

\section{Related Work}

\section{Methodology}
For our prototype adversarial example generator we implemented a simplified version of the algorithm proposed by Samanta and Mehta \cite{samanta_towards_2017}. The inputs are a trained model, and a set of examples which are correctly classified by the model. For each example we apply transformations to the text until it is misclassified by the model, or the maximum number of transformations is reached.

Currently the only type of transformation we have implemented is the deletion of single words. For each word $w_k$ in the text we calculate it's contribution to the predicted probability of the true class ($y_i$):

\begin{equation}
    C(w_k, y_i) = p(y_i | s) - p(y_i | s^{w_k})
\end{equation}

Where $s$ is the original text and $s^{w_k}$ is the text with $w_k$ deleted.

We then remove words from the text in decreasing order of contribution until the model's prediction changes. To limit the distortion of the text we cap the number of allowed deletions. If the prediction does not change before this limit is reached we restore the text to its original state.

This technique does not rely on any knowledge of the structure of the classifier. It must only provide methods for generating feature vectors from the raw data, predicting class probabilities, and there cannot be a limit on the number of predictions.

\section{Experimental Results}
To test our prototype we generated adversarial examples using the FNC-1 baseline model \cite{noauthor_baseline_2018} and the FNC test dataset. We only attempted to transform examples which were correctly classified as \texttt{agree} \texttt{disagree} or \texttt{discuss}. Examples classified as \texttt{unrelated} would be difficult to generate adversarial examples for without adding information to them.

\section{Remaining Work}

\medskip

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
